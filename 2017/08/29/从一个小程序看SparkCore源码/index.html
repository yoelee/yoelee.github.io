<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">
<meta name="google-site-verification" content="u51Tw-7XO-kfvd_YjNzNs4-HpCBkVQe3St8W8bIKI2w" />









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="spark, 源码," />










<meta name="description" content="从一个小程序看 spark core 源码2017-8-29liyakun 从一个小程序的运行入手，看一下整个spark core 底层的流程。 1. 小程序-GroupByTest这是一个使用Spark进行GroupBy的小程序。 程序属于Spark使用代码示例中的一个，在Spark中的源码的位置是：spark&#x2F;spark-branch-2.0&#x2F;examples&#x2F;s">
<meta property="og:type" content="article">
<meta property="og:title" content="从一个小程序看 spark core 源码">
<meta property="og:url" content="https://yoelee.github.io/2017/08/29/%E4%BB%8E%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%A8%8B%E5%BA%8F%E7%9C%8BSparkCore%E6%BA%90%E7%A0%81/index.html">
<meta property="og:site_name" content="亚坤的博客">
<meta property="og:description" content="从一个小程序看 spark core 源码2017-8-29liyakun 从一个小程序的运行入手，看一下整个spark core 底层的流程。 1. 小程序-GroupByTest这是一个使用Spark进行GroupBy的小程序。 程序属于Spark使用代码示例中的一个，在Spark中的源码的位置是：spark&#x2F;spark-branch-2.0&#x2F;examples&#x2F;s">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2017-08-29T02:35:23.000Z">
<meta property="article:modified_time" content="2019-08-23T03:52:34.000Z">
<meta property="article:author" content="亚坤">
<meta property="article:tag" content="spark, 源码">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://yoelee.github.io/2017/08/29/从一个小程序看SparkCore源码/"/>





  <title>从一个小程序看 spark core 源码 | 亚坤的博客</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">亚坤的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://yoelee.github.io/2017/08/29/%E4%BB%8E%E4%B8%80%E4%B8%AA%E5%B0%8F%E7%A8%8B%E5%BA%8F%E7%9C%8BSparkCore%E6%BA%90%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="亚坤的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">从一个小程序看 spark core 源码</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-29T10:35:23+08:00">
                2017-08-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>从一个小程序看 spark core 源码<br>2017-8-29<br>liyakun</p>
<p>从一个小程序的运行入手，看一下整个spark core 底层的流程。</p>
<h1 id="1-小程序-GroupByTest"><a href="#1-小程序-GroupByTest" class="headerlink" title="1. 小程序-GroupByTest"></a>1. 小程序-GroupByTest</h1><p>这是一个使用Spark进行GroupBy的小程序。</p>
<p>程序属于Spark使用代码示例中的一个，在Spark中的源码的位置是：spark&#x2F;spark-branch-2.0&#x2F;examples&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;examples&#x2F;GroupByTest.scala</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">import java.util.Random</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Usage: GroupByTest [numMappers] [numKVPairs] [KeySize] [numReducers]</span><br><span class="line"> */</span><br><span class="line">object GroupByTest &#123;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession.builder.appName(&quot;GroupBy Test&quot;).getOrCreate()</span><br><span class="line"></span><br><span class="line">    val numMappers = if (args.length &gt; 0) args(0).toInt else 2</span><br><span class="line">    val numKVPairs = if (args.length &gt; 1) args(1).toInt else 1000</span><br><span class="line">    val valSize = if (args.length &gt; 2) args(2).toInt else 1000</span><br><span class="line">    val numReducers = if (args.length &gt; 3) args(3).toInt else numMappers</span><br><span class="line"></span><br><span class="line">    val pairs1 = spark.sparkContext.parallelize(0 until numMappers, numMappers).flatMap &#123; p =&gt;</span><br><span class="line">      val ranGen = new Random</span><br><span class="line">      val arr1 = new Array[(Int, Array[Byte])](numKVPairs)</span><br><span class="line">      for (i &lt;- 0 until numKVPairs) &#123;                                                              </span><br><span class="line">        val byteArr = new Array[Byte](valSize)                                                     </span><br><span class="line">        ranGen.nextBytes(byteArr)                                                                  </span><br><span class="line">        arr1(i) = (ranGen.nextInt(Int.MaxValue), byteArr)                                          </span><br><span class="line">      &#125;                                                                                            </span><br><span class="line">      arr1                                                                                         </span><br><span class="line">    &#125;.cache()                                                                                      </span><br><span class="line">    // Enforce that everything has been calculated and in cache                                    </span><br><span class="line">    pairs1.count()                                                                                 </span><br><span class="line">                                                                                                   </span><br><span class="line">    println(pairs1.groupByKey(numReducers).count())                                                </span><br><span class="line">                                                                                                   </span><br><span class="line">    spark.stop()                                                                                   </span><br><span class="line">  &#125;                                                                                                </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="2-切入点"><a href="#2-切入点" class="headerlink" title="2. 切入点"></a>2. 切入点</h1><p>spark切入点是创建sparkSession的部分代码：SparkSession.builder.appName(“GroupBy Test”).getOrCreate()</p>
<p>在Spark的早期版本，sparkContext是进入Spark的切入点。我们都知道RDD是Spark中重要的API，然而它的创建和操作得使用sparkContext提供的API；对于RDD之外的其他东西，我们需要使用其他的Context。比如对于流处理来说，我们得使用StreamingContext；对于SQL得使用sqlContext；而对于hive得使用HiveContext。然而DataSet和Dataframe提供的API逐渐称为新的标准API，我们需要一个切入点来构建它们，所以在 Spark 2.0中引入了一个新的切入点(entry point)：SparkSession</p>
<p>　　SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p>
<p>SparkSession的源码路径为：spark-branch-2.0&#x2F;sql&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;sql&#x2F;SparkSession.scala</p>
<p>查看其中的getOrCreate()方法的具体逻辑，发现，它会先查检是否存在全局的session或者运行中的session，如果都没有，那么先使用用户提供的所有参数创建一个sparkConf，再通过这个sparkConf创建一个sparkContext，然后再使用这个sparkContext创建一个SparkSession。</p>
<p>SparkSession对象中，会包含一个sqlContext，并且可以通过enableHiveSupport方法，来支持对HiveContext的使用。</p>
<h1 id="3-第一个RDD的创建"><a href="#3-第一个RDD的创建" class="headerlink" title="3. 第一个RDD的创建"></a>3. 第一个RDD的创建</h1><p>spark的session对象中有一个SparkContext的成员对象，通过这个SparkContext对象的parallelize方法，可以产生第一个RDD。</p>
<p>下面是这个方法的源码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/SparkContext.scala</span><br><span class="line">   def parallelize[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int = defaultParallelism): RDD[T] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>从上面的源码中，可以看到，这个parallelize方法，最终会产生并返回一个ParallelCollectionRDD类型的RDD对象。</p>
<h1 id="4-RDD的转换"><a href="#4-RDD的转换" class="headerlink" title="4. RDD的转换"></a>4. RDD的转换</h1><p>ParallelCollectionRDD类里面并没有对flatMap()方法进行实现，但是它的父类RDD里面实现了这个方法。</p>
<p>.flatMap()是RDD的一个转换操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/rdd/RDD.scala</span><br><span class="line">  def flatMap[U: ClassTag](f: T =&gt; TraversableOnce[U]): RDD[U] = withScope &#123;</span><br><span class="line">    val cleanF = sc.clean(f)</span><br><span class="line">    new MapPartitionsRDD[U, T](this, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>flatMap这个函数，首先把检查一下函数是否是可以序列化的，然后产生出来一个新的MapPartitionsRDD，并返回这个新的MapPartitionsRDD。接下来，再看一下MapPartitionsRDD的源代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/rdd/MapPartitionsRDD.scala</span><br><span class="line">private[spark] class MapPartitionsRDD[U: ClassTag, T: ClassTag](</span><br><span class="line">    var prev: RDD[T],</span><br><span class="line">    f: (TaskContext, Int, Iterator[T]) =&gt; Iterator[U],  // (TaskContext, partition index, iterator)</span><br><span class="line">    preservesPartitioning: Boolean = false)</span><br><span class="line">  extends RDD[U](prev) &#123;</span><br><span class="line"></span><br><span class="line">  override val partitioner = if (preservesPartitioning) firstParent[T].partitioner else None</span><br><span class="line"></span><br><span class="line">  override def getPartitions: Array[Partition] = firstParent[T].partitions</span><br><span class="line"></span><br><span class="line">  override def compute(split: Partition, context: TaskContext): Iterator[U] =</span><br><span class="line">    f(context, split.index, firstParent[T].iterator(split, context))</span><br><span class="line"></span><br><span class="line">  override def clearDependencies() &#123;</span><br><span class="line">    super.clearDependencies()</span><br><span class="line">    prev = null</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>MapPartitionsRDD继承了RDD这个抽象类，并且实现了自己的partitioner，getPartitions，compute，clearDependencies。这个代码里面最重要的是重写了compute方法。</p>
<p>大家都知道Spark是lazy的计算模型，这个RDD的转换，其实本身并不会立即产生真实的计算，但是RDD的每次转换，都通过把自己的compute叠加起来了，等将来真的需要计算的时候，这些叠加在一起的函数就会开始层层的计算，这个是后话了。</p>
<h1 id="5-RDD持久化"><a href="#5-RDD持久化" class="headerlink" title="5. RDD持久化"></a>5. RDD持久化</h1><p>示例小程序中的.cache()方法是将RDD进行持久化的操作，是属于RDD这个抽象类实现的方法。下面咱们看一下它的源码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/rdd/RDD.scala</span><br><span class="line">  def cache(): this.type = persist()</span><br></pre></td></tr></table></figure>

<h2 id="5-1-定义缓存"><a href="#5-1-定义缓存" class="headerlink" title="5.1 定义缓存"></a>5.1 定义缓存</h2><p>cache调用了persist()方法，然后再看一下persist()方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/rdd/RDD.scala</span><br><span class="line">  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br></pre></td></tr></table></figure>

<p>在简单的几个函数调用之后，会在其中一个persist方法里面，为这个RDD的storageLevel进行赋值，storageLevel &#x3D; newLevel。</p>
<p>再之后，经过经过几个简单的函数调用，最终是调用了SparkContext类的persistRDD方法。这个方法非常简单，就是把rdd以自己的id为key，以自己为value放入到一个名字叫做persistentRdds的map里面。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/SparkContext.scala</span><br><span class="line">  private[spark] def persistRDD(rdd: RDD[_]) &#123;</span><br><span class="line">    persistentRdds(rdd.id) = rdd</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>显然，整个过程，并没有真正的进行RDD的缓存操作，只是记录了已经被标记为persist的RDD的引用.</p>
<p>在定义缓存阶段，实际上只做了两件事：</p>
<ul>
<li>一是设置了rdd的StorageLevel</li>
<li>二是将rdd加到了persistentRdds中并在ContextCleaner中注册</li>
</ul>
<h2 id="5-2-触发缓存"><a href="#5-2-触发缓存" class="headerlink" title="5.2 触发缓存"></a>5.2 触发缓存</h2><p>spark的计算是lazy的，只有在执行action时才真正去计算每个RDD的数据。为了便于理解，这里先提前介绍一下这方面的工作内容。</p>
<p>在Spark的Executor去执行task的计算时，会调用到RDD的iterator方法(详细的代码，可以参见：spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;ResultTask.scala，和，spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;ShuffleMapTask.scala，这两个类里面的runTask方法)，来对RDD的指定partition进行计算。</p>
<p>下面，仔细的看一下RDD的iterator方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/rdd/RDD.scala</span><br><span class="line">  final def iterator(split: Partition, context: TaskContext): Iterator[T] = &#123;</span><br><span class="line">    if (storageLevel != StorageLevel.NONE) &#123;</span><br><span class="line">      getOrCompute(split, context)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      computeOrReadCheckpoint(split, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>首先是判断一下storageLevel是否不为NONE，在之前的定义阶段已经设定为了不为NONE了，因此，继续深入到getOrCompute方法里面：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/rdd/RDD.scala</span><br><span class="line">  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = &#123;</span><br><span class="line">    val blockId = RDDBlockId(id, partition.index)</span><br><span class="line">    var readCachedBlock = true</span><br><span class="line">    // This method is called on executors, so we need call SparkEnv.get instead of sc.env.</span><br><span class="line">    SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () =&gt; &#123;</span><br><span class="line">      readCachedBlock = false</span><br><span class="line">      computeOrReadCheckpoint(partition, context)</span><br><span class="line">    &#125;) match &#123;</span><br><span class="line">      case Left(blockResult) =&gt;</span><br><span class="line">        if (readCachedBlock) &#123;</span><br><span class="line">          val existingMetrics = context.taskMetrics().inputMetrics</span><br><span class="line">          existingMetrics.incBytesRead(blockResult.bytes)</span><br><span class="line">          new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) &#123;</span><br><span class="line">            override def next(): T = &#123;</span><br><span class="line">              existingMetrics.incRecordsRead(1)</span><br><span class="line">              delegate.next()</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]])</span><br><span class="line">        &#125;</span><br><span class="line">      case Right(iter) =&gt;</span><br><span class="line">        new InterruptibleIterator(context, iter.asInstanceOf[Iterator[T]])</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>首先是获取到唯一的blockid，计算的方法非常简单，就是”rdd_” + rddId + “_” + splitIndex。</p>
<p>然后是调用getOrElseUpdate函数，再然后是针对这个函数的返回值进行特殊处理，先看一下getOrElseUpdate函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/storage/BlockManager.scala</span><br><span class="line">    def getOrElseUpdate[T](</span><br><span class="line">      blockId: BlockId,</span><br><span class="line">      level: StorageLevel,</span><br><span class="line">      classTag: ClassTag[T],</span><br><span class="line">      makeIterator: () =&gt; Iterator[T]): Either[BlockResult, Iterator[T]] = &#123;</span><br><span class="line">    // Attempt to read the block from local or remote storage. If it&#x27;s present, then we don&#x27;t need</span><br><span class="line">    // to go through the local-get-or-put path.</span><br><span class="line">    get(blockId) match &#123;</span><br><span class="line">      case Some(block) =&gt;</span><br><span class="line">        return Left(block)</span><br><span class="line">      case _ =&gt;</span><br><span class="line">        // Need to compute the block.</span><br><span class="line">    &#125;</span><br><span class="line">    // Initially we hold no locks on this block.</span><br><span class="line">    doPutIterator(blockId, makeIterator, level, classTag, keepReadLock = true) match &#123;</span><br><span class="line">      case None =&gt;</span><br><span class="line">        // doPut() didn&#x27;t hand work back to us, so the block already existed or was successfully</span><br><span class="line">        // stored. Therefore, we now hold a read lock on the block.</span><br><span class="line">        val blockResult = getLocalValues(blockId).getOrElse &#123;</span><br><span class="line">          // Since we held a read lock between the doPut() and get() calls, the block should not</span><br><span class="line">          // have been evicted, so get() not returning the block indicates some internal error.</span><br><span class="line">          releaseLock(blockId)</span><br><span class="line">          throw new SparkException(s&quot;get() failed for block $blockId even though we held a lock&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">        // We already hold a read lock on the block from the doPut() call and getLocalValues()</span><br><span class="line">        // acquires the lock again, so we need to call releaseLock() here so that the net number</span><br><span class="line">        // of lock acquisitions is 1 (since the caller will only call release() once).</span><br><span class="line">        releaseLock(blockId)</span><br><span class="line">        Left(blockResult)</span><br><span class="line">      case Some(iter) =&gt;</span><br><span class="line">        // The put failed, likely because the data was too large to fit in memory and could not be</span><br><span class="line">        // dropped to disk. Therefore, we need to pass the input iterator back to the caller so</span><br><span class="line">        // that they can decide what to do with the values (e.g. process them without caching).</span><br><span class="line">       Right(iter)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>首先是通过get(blockId)来看这个blockid是否已经存在了，get会先调用getLocal在本地获取，如果本地没有则调用getRemote去远程寻找，如果查到了，就返回Left(block)。如果没有查到，那么就使用doPutIterator方法，把block放入进去。</p>
<h1 id="6-RDD的Action"><a href="#6-RDD的Action" class="headerlink" title="6. RDD的Action"></a>6. RDD的Action</h1><p>Action会真正的启动一个Spark任务，整个计算从Driver触发，然后安排好各个Executor的计算内容，然后把Executor都拉进来，进入真正的分布式计算，最终把结果收集到Driver上面，返回给用户。</p>
<h2 id="6-1-Driver创建TaskSet"><a href="#6-1-Driver创建TaskSet" class="headerlink" title="6.1 Driver创建TaskSet"></a>6.1 Driver创建TaskSet</h2><p>再回到小程的源码里面，pairs1.count()是一个Action类型的操作，pairs1是MapPartitionsRDD类型的，这个类里面并没有count方法，但是它的父类RDD类里面有，下面是它的源码实现。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\rdd\RDD.scala</span><br><span class="line">  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum</span><br></pre></td></tr></table></figure>

<p>可以看到，程序调用了SparkContext的runJob方法，之后层层调用各种runJob方法，一直到spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\DAGScheduler.scala里面的runJob方法。</p>
<p>在这个方法里面，会调用submitJob方法，然后在里面会new一个JobWaiter对象，用来等待此次Job的执行结束。然后就把关于这个Job的所有信息提交给eventProcessLoop队列，等待执行和结束，具体的这块代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\DAGScheduler.scala</span><br><span class="line">    def submitJob[T, U](</span><br><span class="line">      rdd: RDD[T],</span><br><span class="line">      func: (TaskContext, Iterator[T]) =&gt; U,</span><br><span class="line">      partitions: Seq[Int],</span><br><span class="line">      callSite: CallSite,</span><br><span class="line">      resultHandler: (Int, U) =&gt; Unit,</span><br><span class="line">      properties: Properties): JobWaiter[U] = &#123;</span><br><span class="line">    // Check to make sure we are not launching a task on a partition that does not exist.</span><br><span class="line">    val maxPartitions = rdd.partitions.length</span><br><span class="line">    partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt;</span><br><span class="line">      throw new IllegalArgumentException(</span><br><span class="line">        &quot;Attempting to access a non-existent partition: &quot; + p + &quot;. &quot; +</span><br><span class="line">          &quot;Total number of partitions: &quot; + maxPartitions)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val jobId = nextJobId.getAndIncrement()</span><br><span class="line">    if (partitions.size == 0) &#123;</span><br><span class="line">      // Return immediately if the job is running 0 tasks</span><br><span class="line">      return new JobWaiter[U](this, jobId, 0, resultHandler)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    assert(partitions.size &gt; 0)</span><br><span class="line">    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]</span><br><span class="line">    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)</span><br><span class="line">    eventProcessLoop.post(JobSubmitted(</span><br><span class="line">      jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">      SerializationUtils.clone(properties)))</span><br><span class="line">    waiter</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>此时，eventProcessLoop会触发一个onReceive函数，里面会通过dagScheduler.handleJobSubmitted方法来处理JobSubmitted事件。</p>
<p>在dagScheduler.handleJobSubmitted中，第一个要处理的，就是创建能产生最终结果RDD的Stage：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\DAGScheduler.scala</span><br><span class="line">    private[scheduler] def handleJobSubmitted(jobId: Int,</span><br><span class="line">      finalRDD: RDD[_],</span><br><span class="line">      func: (TaskContext, Iterator[_]) =&gt; _,</span><br><span class="line">      partitions: Array[Int],</span><br><span class="line">      callSite: CallSite,</span><br><span class="line">      listener: JobListener,</span><br><span class="line">      properties: Properties) &#123;</span><br><span class="line">    var finalStage: ResultStage = null</span><br><span class="line">    try &#123;</span><br><span class="line">      // New stage creation may throw an exception if, for example, jobs are run on a</span><br><span class="line">      // HadoopRDD whose underlying HDFS files have been deleted.</span><br><span class="line">      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">        logWarning(&quot;Creating new stage failed due to exception - job: &quot; + jobId, e)</span><br><span class="line">        listener.jobFailed(e)</span><br><span class="line">        return</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">    clearCacheLocs()</span><br><span class="line">    logInfo(&quot;Got job %s (%s) with %d output partitions&quot;.format(</span><br><span class="line">      job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">    logInfo(&quot;Final stage: &quot; + finalStage + &quot; (&quot; + finalStage.name + &quot;)&quot;)</span><br><span class="line">    logInfo(&quot;Parents of final stage: &quot; + finalStage.parents)</span><br><span class="line">    logInfo(&quot;Missing parents: &quot; + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">    val jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">    jobIdToActiveJob(jobId) = job</span><br><span class="line">    activeJobs += job</span><br><span class="line">    finalStage.setActiveJob(job)</span><br><span class="line">    val stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">    val stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">    listenerBus.post(</span><br><span class="line">      SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">    submitStage(finalStage)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>然后，在这个方法里面，会创建一个ActiveJob对象，对把这个对象跟finalStage绑定起来，最后，调用了一个方法submitStage(finalStage)。然后就进入了submitStage(finalStage)方面里面，源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\DAGScheduler.scala</span><br><span class="line">  private def submitStage(stage: Stage) &#123;</span><br><span class="line">    val jobId = activeJobForStage(stage)</span><br><span class="line">    if (jobId.isDefined) &#123;</span><br><span class="line">      logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;)</span><br><span class="line">      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">        val missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">        logDebug(&quot;missing: &quot; + missing)</span><br><span class="line">        if (missing.isEmpty) &#123;</span><br><span class="line">          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)</span><br><span class="line">          submitMissingTasks(stage, jobId.get)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          for (parent &lt;- missing) &#123;</span><br><span class="line">            submitStage(parent)</span><br><span class="line">          &#125;</span><br><span class="line">          waitingStages += stage</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>submitStage方法会被递归的调用，初始值是最终的结果Stage，最终的目标是提交上所有的Stage。注意里面的getMissingParentStages方法，这个方法的的目标是为了找到为了获取当前的Stage，所依赖的前面的Stage，源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\DAGScheduler.scala</span><br><span class="line">  private def getMissingParentStages(stage: Stage): List[Stage] = &#123;</span><br><span class="line">    val missing = new HashSet[Stage]</span><br><span class="line">    val visited = new HashSet[RDD[_]]</span><br><span class="line">    // We are manually maintaining a stack here to prevent StackOverflowError</span><br><span class="line">    // caused by recursively visiting</span><br><span class="line">    val waitingForVisit = new Stack[RDD[_]]</span><br><span class="line">    def visit(rdd: RDD[_]) &#123;</span><br><span class="line">      if (!visited(rdd)) &#123;</span><br><span class="line">        visited += rdd</span><br><span class="line">        val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)</span><br><span class="line">        if (rddHasUncachedPartitions) &#123;</span><br><span class="line">          for (dep &lt;- rdd.dependencies) &#123;</span><br><span class="line">            dep match &#123;</span><br><span class="line">              case shufDep: ShuffleDependency[_, _, _] =&gt;</span><br><span class="line">                val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)</span><br><span class="line">                if (!mapStage.isAvailable) &#123;</span><br><span class="line">                  missing += mapStage</span><br><span class="line">                &#125;</span><br><span class="line">              case narrowDep: NarrowDependency[_] =&gt;</span><br><span class="line">                waitingForVisit.push(narrowDep.rdd)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    waitingForVisit.push(stage.rdd)</span><br><span class="line">    while (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">      visit(waitingForVisit.pop())</span><br><span class="line">    &#125;</span><br><span class="line">    missing.toList</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>getMissingParentStages方法里面，以当前要处理的stage的rdd为根，依托一个stack，以一个类似于按层遍历的方式，把当前的rdd依赖的所有rdd遍历一遍。但是有一点与按层遍历不同的是，这里面，如果依赖的类型是ShuffleDependency的话，那么将不再进行更深层的遍历，会在这里增加一个Stage，并把这些增加的所有的Stage收集起来，用来返回。</p>
<p>在submitStage函数中，收到getMissingParentStages返回的Stage集之后，会对每个返回的结果，再次递归的调用submitStage函数，如果getMissingParentStages返回为空的话，那么意味着此时已经走到了Stage的最前面，这时，会调用submitMissingTasks方法，这个一会儿再说。</p>
<p><strong>先总结一下，Stage的划分过程是，从最终结果Stage开始，依次往前推，遇到ShuffleDependency就产生一个新的Stage。</strong></p>
<p>下面沿着刚刚的线继续往下走，如果getMissingParentStages返回为空的话(在递归的过程中可能会有多个getMissingParentStages的返回为空的情况)，那么意味着此时已经走到了Stage的最前面，这时，会调用submitMissingTasks方法。然后，咱们进入submitMissingTasks方法。</p>
<p>不过，首先有一点需要了解一下，那就是在Spark内部，只有两种类型的Stage，那就是ShuffleMapStage和ResultStage:</p>
<ul>
<li>ShuffleMapStage <ul>
<li>这种Stage是以Shuffle为输出边界</li>
<li>其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出</li>
<li>其输出是另一个Stage的开始</li>
<li>ShuffleMapStage的最后Task就是ShuffleMapTask</li>
<li>在一个Job里可能有该类型的Stage，也可以能没有该类型Stage。</li>
</ul>
</li>
<li>ResultStage <ul>
<li>这种Stage是直接输出结果</li>
<li>其输入边界可以是从外部获取数据，也可以是另一个ShuffleMapStage的输出</li>
<li>ResultStage的最后Task就是ResultTask</li>
<li>在一个Job里必定有该类型Stage。</li>
</ul>
</li>
</ul>
<p>submitMissingTasks方法里面，会考虑数据的本地性，为每个Task选择自己计算的最好的位置，这个以后再说。先假定计算的位置已经选好，接下来会通过序列化产生task所需要的二进制文件，并通过sc.broadcast(taskBinaryBytes)广播出去。然后，如果是ShuffleMapStage会产生ShuffleMapTask类型的TaskSet，如果是ResultStage会产生ResultTask类型的TaskSet。最终，通过如下的方法，提交TaskSet给taskScheduler。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\DAGScheduler.scala</span><br><span class="line">taskScheduler.submitTasks(new TaskSet(</span><br><span class="line">    tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))</span><br></pre></td></tr></table></figure>

<p>下面深入到TaskScheduler的具体代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala</span><br><span class="line"></span><br><span class="line">  override def submitTasks(taskSet: TaskSet) &#123;</span><br><span class="line">    val tasks = taskSet.tasks</span><br><span class="line">    logInfo(&quot;Adding task set &quot; + taskSet.id + &quot; with &quot; + tasks.length + &quot; tasks&quot;)</span><br><span class="line">    this.synchronized &#123;</span><br><span class="line">      val manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">      val stage = taskSet.stageId</span><br><span class="line">      val stageTaskSets =</span><br><span class="line">        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])</span><br><span class="line">      stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">      val conflictingTaskSet = stageTaskSets.exists &#123; case (_, ts) =&gt;</span><br><span class="line">        ts.taskSet != taskSet &amp;&amp; !ts.isZombie</span><br><span class="line">      &#125;</span><br><span class="line">      if (conflictingTaskSet) &#123;</span><br><span class="line">        throw new IllegalStateException(s&quot;more than one active taskSet for stage $stage:&quot; +</span><br><span class="line">          s&quot; $&#123;stageTaskSets.toSeq.map&#123;_._2.taskSet.id&#125;.mkString(&quot;,&quot;)&#125;&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"></span><br><span class="line">      if (!isLocal &amp;&amp; !hasReceivedTask) &#123;</span><br><span class="line">        starvationTimer.scheduleAtFixedRate(new TimerTask() &#123;</span><br><span class="line">          override def run() &#123;</span><br><span class="line">            if (!hasLaunchedTask) &#123;</span><br><span class="line">              logWarning(&quot;Initial job has not accepted any resources; &quot; +</span><br><span class="line">                &quot;check your cluster UI to ensure that workers are registered &quot; +</span><br><span class="line">                &quot;and have sufficient resources&quot;)</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">              this.cancel()</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)</span><br><span class="line">      &#125;</span><br><span class="line">      hasReceivedTask = true</span><br><span class="line">    &#125;</span><br><span class="line">    backend.reviveOffers()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>TaskSchedulerImpl的submitTasks方法首先创建TaskSetManager，TaskSetManager负责管理TaskSchedulerImpl中一个单独TaskSet，跟踪每一个task，如果task失败，负责重试task直到达到task重试次数的最多次数。并且通过延迟调度来执行task的位置感知调度。接下来，会把manager添加到schedulableBuilder里面。</p>
<p>由schedulableBuilder决定调度顺序,schedulableBuilder的类型是 SchedulerBuilder，SchedulerBuilder是一个trait，有两个实现FIFOSchedulerBuilder和 FairSchedulerBuilder，并且默认采用的是FIFO方式。</p>
<p>而schedulableBuilder的创建是在SparkContext创建SchedulerBackend和TaskScheduler后调用TaskSchedulerImpl的初始化方法进行创建的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala</span><br><span class="line">  def initialize(backend: SchedulerBackend) &#123;</span><br><span class="line">    this.backend = backend</span><br><span class="line">    // temporarily set rootPool name to empty</span><br><span class="line">    rootPool = new Pool(&quot;&quot;, schedulingMode, 0, 0)</span><br><span class="line">    schedulableBuilder = &#123;</span><br><span class="line">      schedulingMode match &#123;</span><br><span class="line">        case SchedulingMode.FIFO =&gt;</span><br><span class="line">          new FIFOSchedulableBuilder(rootPool)</span><br><span class="line">        case SchedulingMode.FAIR =&gt;</span><br><span class="line">          new FairSchedulableBuilder(rootPool, conf)</span><br><span class="line">        case _ =&gt;</span><br><span class="line">          throw new IllegalArgumentException(s&quot;Unsupported spark.scheduler.mode: $schedulingMode&quot;)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    schedulableBuilder.buildPools()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>schedulableBuilder是TaskScheduler中一个重要成员，他根据调度策略决定了TaskSetManager的调度顺序。</p>
<p>接下来，回归submitTasks方法，最后一行，调用了SchedulerBackend的riviveOffers方法对Task进行调度，决定task具体运行在哪个Executor中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala</span><br><span class="line"></span><br><span class="line">backend.reviveOffers()</span><br></pre></td></tr></table></figure>

<p>其中，backend是SchedulerBackend的实例，这个对象是在SparkContext实例创建时，通过createTaskScheduler方法创建出的，在这个方法里面，会根据master的名字的不同，创建出来不同类型的SchedulerBackend，具体的代码位置是：SparkContext类里面的createTaskScheduler方法。在这里咱只深入了解一下master为yarn的情况，master为yarn是一种比较特殊的情况，它需要通过getClusterManager方法来load外部的ExternalClusterManager，一旦load成功，就开始创建scheduler和backend，并为它们进行初始化，代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/SparkContext.scala</span><br><span class="line">      case masterUrl =&gt;</span><br><span class="line">        val cm = getClusterManager(masterUrl) match &#123;</span><br><span class="line">          case Some(clusterMgr) =&gt; clusterMgr</span><br><span class="line">          case None =&gt; throw new SparkException(&quot;Could not parse Master URL: &#x27;&quot; + master + &quot;&#x27;&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">        try &#123;</span><br><span class="line">          val scheduler = cm.createTaskScheduler(sc, masterUrl)</span><br><span class="line">          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)</span><br><span class="line">          cm.initialize(scheduler, backend)</span><br><span class="line">          (backend, scheduler)</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case se: SparkException =&gt; throw se</span><br><span class="line">          case NonFatal(e) =&gt;</span><br><span class="line">            throw new SparkException(&quot;External scheduler cannot be instantiated&quot;, e)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>这两个对象是SparkContext中非常重要的对象。scheduler负责将本地的计算任务进行Stage划分和TaskSet生成。backend负责跟其它的executor交互。</p>
<p>在这里，先继续回归到刚才的话题，看一下backend,在master为yarn的情况下，使用client模式时，backend的类型为YarnClientSchedulerBackend，YarnClientSchedulerBackend里面只有简单的几个覆盖实现，大部分的方法需要深入到它继承的YarnSchedulerBackend里面才能看到，YarnSchedulerBackend里面也没有reviveOffers方法的实现，需要再深入一层，到它继承的CoarseGrainedSchedulerBackend方法里面，看到这个类里面确实是有reviveOffers方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\cluster\CoarseGrainedSchedulerBackend.scala</span><br><span class="line">  override def reviveOffers() &#123;</span><br><span class="line">    driverEndpoint.send(ReviveOffers)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>它调用了driverEndpoint.send(ReviveOffers)方法，这个driverEndpoint是一个NettyRpcEnv的实例，它的类的位置是：spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;rpc&#x2F;netty&#x2F;NettyRpcEnv.scala。</p>
<p>RpcEnv是各个组件之间通信的执行环境，每个节点之间（Driver或者Worker）组件的Endpoint和对应的EndpointRef之间的信息通信和方法调用都是通过RpcEnv作协调，而底层是通过Netty NIO框架实现（Spark早期版本通信是通过Akka，大的文件传输是通过Netty，在2.0.0版本后统一由Netty替换成了Akka，实现了通信传输统一化）</p>
<h2 id="6-2-Executor执行Task"><a href="#6-2-Executor执行Task" class="headerlink" title="6.2 Executor执行Task"></a>6.2 Executor执行Task</h2><p>上回讲到，Driver把自己的Task序列化后，通过RPC远程发给Executor。Executor的接收入口在spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;executor&#x2F;CoarseGrainedExecutorBackend.scala类里面。在接收到消息之后，它会调用receiver方法，里面会判断到是LaunchTask类型的消息，然后在反序列化之后调用executor.launchTask方法加载Task。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\executor\CoarseGrainedExecutorBackend.scala</span><br><span class="line">    case LaunchTask(data) =&gt;</span><br><span class="line">      if (executor == null) &#123;</span><br><span class="line">        exitExecutor(1, &quot;Received LaunchTask command but executor was null&quot;)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        val taskDesc = ser.deserialize[TaskDescription](data.value)</span><br><span class="line">        logInfo(&quot;Got assigned task &quot; + taskDesc.taskId)</span><br><span class="line">        executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,</span><br><span class="line">          taskDesc.name, taskDesc.serializedTask)</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure>

<p>Executor里面的launchTask方法，也比较简单，就是把新建立一个TaskRunner对象用来把Task封装一下，然后记录一下，交给线程池去运行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\executor\Executor.scala</span><br><span class="line">  def launchTask(</span><br><span class="line">      context: ExecutorBackend,</span><br><span class="line">      taskId: Long,</span><br><span class="line">      attemptNumber: Int,</span><br><span class="line">      taskName: String,</span><br><span class="line">      serializedTask: ByteBuffer): Unit = &#123;</span><br><span class="line">    val tr = new TaskRunner(context, taskId = taskId, attemptNumber = attemptNumber, taskName,</span><br><span class="line">      serializedTask)</span><br><span class="line">    runningTasks.put(taskId, tr)</span><br><span class="line">    threadPool.execute(tr)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>下面，需要重点看一下TaskRunner类里面的run方法，位置是：spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;executor&#x2F;Executor.scala里面的内部类TaskRunner，内部的run方法。</p>
<p>首先是新建了一个taskMemoryManager对象，这个之后再分析，然后execBackend.statusUpdate(taskId, TaskState.RUNNING, EMPTY_BYTE_BUFFER)给driver发个消息，task要开始了，然后把传过来的task进行反序列化，设置好taskMemoryManager等信息，然后向mapOutputTracker更新一下时间截，紧接着，调用了task本身对象的run方法来获得task的结果，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\executor\Executor.scala</span><br><span class="line">          val res = task.run(</span><br><span class="line">            taskAttemptId = taskId,</span><br><span class="line">            attemptNumber = attemptNumber,</span><br><span class="line">            metricsSystem = env.metricsSystem)</span><br></pre></td></tr></table></figure>

<p>在继续讨论TaskRunner内部的run方法里面的后续步骤之前，先稍微深入一点，讨论一下这里的task.run方法，针对于不同类型的Task是有不同的逻辑的。</p>
<ul>
<li>ResultTask会返回它的函数的计算结果；</li>
<li>ShuffleMapTask会返回MapStatus。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//ResultTask</span><br><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\ResultTask.scala</span><br><span class="line">override def runTask(context: TaskContext): U = &#123;</span><br><span class="line">    // Deserialize the RDD and the func using the broadcast variables.</span><br><span class="line">    val deserializeStartTime = System.currentTimeMillis()</span><br><span class="line">    val ser = SparkEnv.get.closureSerializer.newInstance()</span><br><span class="line">    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) =&gt; U)](</span><br><span class="line">      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)</span><br><span class="line">    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime</span><br><span class="line"></span><br><span class="line">    func(context, rdd.iterator(partition, context))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">//ShuffleMapTask</span><br><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\scheduler\ShuffleMapTask.scala</span><br><span class="line">  override def runTask(context: TaskContext): MapStatus = &#123;</span><br><span class="line">    // Deserialize the RDD using the broadcast variable.</span><br><span class="line">    val deserializeStartTime = System.currentTimeMillis()</span><br><span class="line">    val ser = SparkEnv.get.closureSerializer.newInstance()</span><br><span class="line">    val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](</span><br><span class="line">      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)</span><br><span class="line">    _executorDeserializeTime = System.currentTimeMillis() - deserializeStartTime</span><br><span class="line"></span><br><span class="line">    var writer: ShuffleWriter[Any, Any] = null</span><br><span class="line">    try &#123;</span><br><span class="line">      val manager = SparkEnv.get.shuffleManager</span><br><span class="line">      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)</span><br><span class="line">      writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &lt;: Product2[Any, Any]]])</span><br><span class="line">      writer.stop(success = true).get</span><br><span class="line">    &#125; catch &#123;</span><br><span class="line">      case e: Exception =&gt;</span><br><span class="line">        try &#123;</span><br><span class="line">          if (writer != null) &#123;</span><br><span class="line">            writer.stop(success = false)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: Exception =&gt;</span><br><span class="line">            log.debug(&quot;Could not stop writer&quot;, e)</span><br><span class="line">        &#125;</span><br><span class="line">        throw e</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="6-2-1-ShuffleMapTask的结果写入-Shuffle-Write"><a href="#6-2-1-ShuffleMapTask的结果写入-Shuffle-Write" class="headerlink" title="6.2.1 ShuffleMapTask的结果写入(Shuffle Write)"></a>6.2.1 ShuffleMapTask的结果写入(Shuffle Write)</h3><p>关于ShuffleMapTask，还有更多的内容要介绍一下。因为ShuffleMapTask是一个Stage的终结，同时它产生的结果，会作为下一个Stage的Shuffle Read的输入，我们有必要关心一下，它的结果的存储方式。</p>
<p>在上面的ShuffleMapTask的runTask方法中，SparkEnv.get.shuffleManager会产生一个SortShuffleManager对象(目前只有这一种shuffleManager)，这个类的位置是：spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;shuffle&#x2F;sort&#x2F;SortShuffleManager.scala</p>
<p>看一下它的getWriter方法，源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\shuffle\sort\SortShuffleManager.scala</span><br><span class="line">  override def getWriter[K, V](</span><br><span class="line">      handle: ShuffleHandle,</span><br><span class="line">      mapId: Int,</span><br><span class="line">      context: TaskContext): ShuffleWriter[K, V] = &#123;</span><br><span class="line">    numMapsForShuffle.putIfAbsent(</span><br><span class="line">      handle.shuffleId, handle.asInstanceOf[BaseShuffleHandle[_, _, _]].numMaps)</span><br><span class="line">    val env = SparkEnv.get</span><br><span class="line">    handle match &#123;</span><br><span class="line">      case unsafeShuffleHandle: SerializedShuffleHandle[K @unchecked, V @unchecked] =&gt;</span><br><span class="line">        new UnsafeShuffleWriter(</span><br><span class="line">          env.blockManager,</span><br><span class="line">          shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver],</span><br><span class="line">          context.taskMemoryManager(),</span><br><span class="line">          unsafeShuffleHandle,</span><br><span class="line">          mapId,</span><br><span class="line">          context,</span><br><span class="line">          env.conf)</span><br><span class="line">      case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =&gt;</span><br><span class="line">        new BypassMergeSortShuffleWriter(</span><br><span class="line">          env.blockManager,</span><br><span class="line">          shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver],</span><br><span class="line">          bypassMergeSortHandle,</span><br><span class="line">          mapId,</span><br><span class="line">          context,</span><br><span class="line">          env.conf)</span><br><span class="line">      case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =&gt;</span><br><span class="line">        new SortShuffleWriter(shuffleBlockResolver, other, mapId, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，这里有三种类型的writer，分别是：</p>
<ul>
<li><p>UnsafeShuffleWriter：在序列化排序模型，当输入的记录被传到shuffle 写入器时会被立即序列化，并且在排序过程中以序列化的格式在缓冲器中。这会减少memory占用和GC开销。在排序过程中，它提供cache-efficient sorter，使用一个8 bytes的指针，把排序转化成了一个指针数组的排序，极大的优化了排序性能。</p>
<ul>
<li>优点：能极大的减少内存占用和GC开销，提高效率</li>
<li>缺点：需要满足三个条件：序列化支持对序列化值的重定位、依赖没有聚集、生成的输出分区小于16777216个。像reduceByKey这类有aggregate操作的算子是不能使用Unsafe Shuffle，它会退化采用Sort Shuffle。</li>
</ul>
</li>
<li><p>BypassMergeSortShuffleWriter：是带Hash风格的基于Sort的Shuffle机制，为每个Reduce端的任务构建一个输出文件，将输入的每条记录分别写入各自对应的文件中，并在最后将这些基于各个分区的文件合并成一个输出文件。</p>
<ul>
<li>优点：没有partition内部的排序，在小数据量的情况下会比较快</li>
<li>缺点：假设一个executor有K个核，下游会有R个reduce，同时打开的文件的个数为K*R，在数量量特别大时，R值会变大，导致不可计算。</li>
</ul>
</li>
<li><p>SortShuffleWriter：在map阶段(shuffle write)，会按照partition id以及key对记录进行排序，将所有partition的数据写在同一个文件中，该文件中的记录首先是按照partition id排序一个一个分区的顺序排列，每个partition内部是按照key进行排序存放，map task运行期间会顺序写每个partition的数据，并通过一个索引文件记录每个partition的大小和偏移量。这样一来，每个map task一次只开两个文件描述符，一个写数据，一个写索引，大大减轻了Hash Shuffle大量文件描述符的问题，即使一个executor有K个core，那么最多一次性开2K个文件描述符。</p>
<ul>
<li>优点：能处理任意规模的数据</li>
<li>缺点：在满足上面两个的条件下，计算的效率比上面两个都要低</li>
</ul>
</li>
</ul>
<p>通过handle的类型来选择对应的writer，那么不禁要问了，如何handle是如何确定的呢？<br>可以看一下在ShuffledRDD对象内部的getDependencies方法内，会创建ShuffleDependency对象，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala</span><br><span class="line">  override def getDependencies: Seq[Dependency[_]] = &#123;</span><br><span class="line">    val serializer = userSpecifiedSerializer.getOrElse &#123;</span><br><span class="line">      val serializerManager = SparkEnv.get.serializerManager</span><br><span class="line">      if (mapSideCombine) &#123;</span><br><span class="line">        serializerManager.getSerializer(implicitly[ClassTag[K]], implicitly[ClassTag[C]])</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        serializerManager.getSerializer(implicitly[ClassTag[K]], implicitly[ClassTag[V]])</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    List(new ShuffleDependency(prev, part, serializer, keyOrdering, aggregator, mapSideCombine))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>ShuffleDependency对象的构造函数里面会创建自己的shuffleHandle，这个创建shuffleHandle的方法是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/Dependency.scala</span><br><span class="line">  val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle(</span><br><span class="line">    shuffleId, _rdd.partitions.length, this)</span><br></pre></td></tr></table></figure>

<p>上面的源码中shuffleManager的实例是SortShuffleManager(目前只有这一种shuffleManager)，接下来，继续沿着代码跟进到SortShuffleManager的registerShuffle方法，源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala</span><br><span class="line">  override def registerShuffle[K, V, C](</span><br><span class="line">      shuffleId: Int,</span><br><span class="line">      numMaps: Int,</span><br><span class="line">      dependency: ShuffleDependency[K, V, C]): ShuffleHandle = &#123;</span><br><span class="line">    if (SortShuffleWriter.shouldBypassMergeSort(SparkEnv.get.conf, dependency)) &#123;</span><br><span class="line">      // If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don&#x27;t</span><br><span class="line">      // need map-side aggregation, then write numPartitions files directly and just concatenate</span><br><span class="line">      // them at the end. This avoids doing serialization and deserialization twice to merge</span><br><span class="line">      // together the spilled files, which would happen with the normal code path. The downside is</span><br><span class="line">      // having multiple files open at a time and thus more memory allocated to buffers.</span><br><span class="line">      new BypassMergeSortShuffleHandle[K, V](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])</span><br><span class="line">    &#125; else if (SortShuffleManager.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">      // Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:</span><br><span class="line">      new SerializedShuffleHandle[K, V](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // Otherwise, buffer map outputs in a deserialized form:</span><br><span class="line">      new BaseShuffleHandle(shuffleId, numMaps, dependency)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在上面的这段代码中，就能看到三种Handle的选择过程了。</p>
<ul>
<li>首先是在不需要mapSideCombine，并且依赖的Partition的数量小于一个配置值：spark.shuffle.sort.bypassMergeThreshold(此值默认200)时，就返回BypassMergeSortShuffleHandle对象</li>
<li>其次如果同时满足下面的三个条件，返回SerializedShuffleHandle<ul>
<li>Shuffle序列化支持对序列化值的重定位(KryoSerializer支持)</li>
<li>Shuffle依赖没有聚集。因为序列化的数据无法直接做聚集操作。</li>
<li>Shuffle生成的输出分区小于16777216个。由于只给partition寻址字段留出了24位的空间。</li>
</ul>
</li>
<li>最后在以上两种情况都不满足时，返回默认的BaseShuffleHandle</li>
</ul>
<h4 id="6-2-1-1-SortShuffleWriter"><a href="#6-2-1-1-SortShuffleWriter" class="headerlink" title="6.2.1.1 SortShuffleWriter"></a>6.2.1.1 SortShuffleWriter</h4><p>然后，再回头看一下getWriter方法。</p>
<p>三种ShufflerWriter中的SortShuffleWriter是最通用的方法，首先看一下如果使用的是SortShuffleWriter，写操作write函数应该是什么样子的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\shuffle\sort\SortShuffleWriter.scala</span><br><span class="line">  override def write(records: Iterator[Product2[K, V]]): Unit = &#123;</span><br><span class="line">    sorter = if (dep.mapSideCombine) &#123;</span><br><span class="line">      require(dep.aggregator.isDefined, &quot;Map-side combine without Aggregator specified!&quot;)</span><br><span class="line">      new ExternalSorter[K, V, C](</span><br><span class="line">        context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // In this case we pass neither an aggregator nor an ordering to the sorter, because we don&#x27;t</span><br><span class="line">      // care whether the keys get sorted in each partition; that will be done on the reduce side</span><br><span class="line">      // if the operation being run is sortByKey.</span><br><span class="line">      new ExternalSorter[K, V, V](</span><br><span class="line">        context, aggregator = None, Some(dep.partitioner), ordering = None, dep.serializer)</span><br><span class="line">    &#125;</span><br><span class="line">    sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line">    // Don&#x27;t bother including the time to open the merged output file in the shuffle write time,</span><br><span class="line">    // because it just opens a single file, so is typically too fast to measure accurately</span><br><span class="line">    // (see SPARK-3570).</span><br><span class="line">    val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">    val tmp = Utils.tempFileWith(output)</span><br><span class="line">    val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)</span><br><span class="line">    val partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class="line">    mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这个方法，首先根据是否需要dep.mapSideCombine，使用不同的方式，创建了一个ExternalSorter的对象，然后调用了sorter.insertAll(records)方法，来把所有的record写出(这一步非常重要，一会儿会详细的介绍)，然后通过sorter.writePartitionedFile方法把数据写入到data文件中，最后再写好Index文件，最终只产生两个文件。大体的过程如此，下面仔细的分析一下，首先是sorter.insertAll(records)方法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\util\collection\ExternalSorter.scala</span><br><span class="line">  def insertAll(records: Iterator[Product2[K, V]]): Unit = &#123;</span><br><span class="line">    // TODO: stop combining if we find that the reduction factor isn&#x27;t high</span><br><span class="line">    val shouldCombine = aggregator.isDefined</span><br><span class="line"></span><br><span class="line">    if (shouldCombine) &#123;</span><br><span class="line">      // Combine values in-memory first using our AppendOnlyMap</span><br><span class="line">      val mergeValue = aggregator.get.mergeValue</span><br><span class="line">      val createCombiner = aggregator.get.createCombiner</span><br><span class="line">      var kv: Product2[K, V] = null</span><br><span class="line">      val update = (hadValue: Boolean, oldValue: C) =&gt; &#123;</span><br><span class="line">        if (hadValue) mergeValue(oldValue, kv._2) else createCombiner(kv._2)</span><br><span class="line">      &#125;</span><br><span class="line">      while (records.hasNext) &#123;</span><br><span class="line">        addElementsRead()</span><br><span class="line">        kv = records.next()</span><br><span class="line">        map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class="line">        maybeSpillCollection(usingMap = true)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // Stick values into our buffer</span><br><span class="line">      while (records.hasNext) &#123;</span><br><span class="line">        addElementsRead()</span><br><span class="line">        val kv = records.next()</span><br><span class="line">        buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[C])</span><br><span class="line">        maybeSpillCollection(usingMap = false)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>首先是根据之前创建时传入的aggregator来分开：</p>
<ul>
<li>先看需要Combine的，update是需要的聚合函数，然后通过while循环来遍历每一条记录，在循环中，首先是读取的数目加1，然后得到当前的kv，然后把这个kv插入到map中(这是一个Spark自己写的PartitionedAppendOnlyMap类型的map)，插入的规则是，先判断一下这个map中是否已经包含此k，如果包含，那跟通过聚合函数进行一次结果合并，如果不包含，则在map中对此k进行初始化。最后，调用一下maybeSpillCollection函数来确认一下是否需要spill到磁盘上。下面仔细分析一下maybeSpillCollection方法：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\util\collection\ExternalSorter.scala</span><br><span class="line">  private def maybeSpillCollection(usingMap: Boolean): Unit = &#123;</span><br><span class="line">    var estimatedSize = 0L</span><br><span class="line">    if (usingMap) &#123;</span><br><span class="line">      estimatedSize = map.estimateSize()</span><br><span class="line">      if (maybeSpill(map, estimatedSize)) &#123;</span><br><span class="line">        map = new PartitionedAppendOnlyMap[K, C]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      estimatedSize = buffer.estimateSize()</span><br><span class="line">      if (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class="line">        buffer = new PartitionedPairBuffer[K, C]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class="line">      _peakMemoryUsedBytes = estimatedSize</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>首先估计map可能需要占用的内存空间，然后通过maybeSpill来判断是否需要spill</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\util\collection\Spillable.scala</span><br><span class="line">  protected def maybeSpill(collection: C, currentMemory: Long): Boolean = &#123;</span><br><span class="line">    var shouldSpill = false</span><br><span class="line">    if (elementsRead % 32 == 0 &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">      // Claim up to double our current memory from the shuffle memory pool</span><br><span class="line">      val amountToRequest = 2 * currentMemory - myMemoryThreshold</span><br><span class="line">      val granted = acquireMemory(amountToRequest)</span><br><span class="line">      myMemoryThreshold += granted</span><br><span class="line">      // If we were granted too little memory to grow further (either tryToAcquire returned 0,</span><br><span class="line">      // or we already had more memory than myMemoryThreshold), spill the current collection</span><br><span class="line">      shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">    &#125;</span><br><span class="line">    shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">    // Actually spill</span><br><span class="line">    if (shouldSpill) &#123;</span><br><span class="line">      _spillCount += 1</span><br><span class="line">      logSpillage(currentMemory)</span><br><span class="line">      spill(collection)</span><br><span class="line">      _elementsRead = 0</span><br><span class="line">      _memoryBytesSpilled += currentMemory</span><br><span class="line">      releaseMemory()</span><br><span class="line">    &#125;</span><br><span class="line">    shouldSpill</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这个方法内部，首先是在读取的条目数量为32的倍数并且当前预估的内存使用大于配置spark.shuffle.spill.initialMemoryThreshold(默认是5 * 1024 * 1024)，先尝试申请更多的内存(这里的内存是Executor中的用于计算的内存)，如果申请不到，或者不够预估的，那么shouldSpill为true。并且，如果当前读取的条目的数量已经比配置值spark.shuffle.spill.numElementsForceSpillThreshold(默认为Long.MaxValue)还要多时，shouldSpill为true。</p>
<p>如果上面得到的shouldSpill为true，那么就会真正的触发一个spill(collection)，把当前的map的内容写入到磁盘文件中，把磁盘文件信息追加到spills这个列表中，然后清空自己的内存使用，重新new一个空白的PartitionedAppendOnlyMap。</p>
<ul>
<li>再回到insertAll里面看不需要combine的情况。它是直接遍历所有记录，每一次循环计数加1，然后得到当前的记录，然后把记录插入到buffer中(这里的buffer是PartitionedPairBuffer类型的，这个数据结构的不同，是是否需要conbine的主要的不同之处)。然后也是判断一下是否需要Spill。整体的过程非常类似，就不再赘述了。</li>
</ul>
<p>在整个过程结束之后，通过sorter.writePartitionedFile方法把数据写入到data文件中，最后再写好Index文件，最终只产生两个文件。整个效果分析一下就是，在执行的过程中，每隔一段时间，会产生一个临时文件，文件里面是排好序的，然后最后归并一下。在过程之中，最多会同时产生2K个文件，其中K是executor的核数，在过程结束时，会只保留两个文件。</p>
<h4 id="6-2-1-2-UnsafeShuffleWriter"><a href="#6-2-1-2-UnsafeShuffleWriter" class="headerlink" title="6.2.1.2 UnsafeShuffleWriter"></a>6.2.1.2 UnsafeShuffleWriter</h4><p>该writer可将数据序列化后写入到堆外内存,只需要按照partitionid对地址进行排序,整个过程不涉及反序列化。</p>
<p>条件：  </p>
<ol>
<li>使用的序列化类需要支持relocation，是指Serializer可以对已经序列化的对象进行排序，这种排序起到的效果和先对数据排序再序列化一致.目前只能使用kryoSerializer。</li>
<li>不需要map side aggregate即不能定义aggregator</li>
<li>partition数量不能大于支持的上限(2^24),由于只给partition寻址字段留出了24位的空间</li>
</ol>
<p>如果使用的是UnsafeShuffleWriter，它的写操作是什么样子的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\UnsafeShuffleWriter.java</span><br><span class="line">  public void write(scala.collection.Iterator&lt;Product2&lt;K, V&gt;&gt; records) throws IOException &#123;</span><br><span class="line">    // Keep track of success so we know if we encountered an exception</span><br><span class="line">    // We do this rather than a standard try/catch/re-throw to handle</span><br><span class="line">    // generic throwables.</span><br><span class="line">    boolean success = false;</span><br><span class="line">    try &#123;</span><br><span class="line">      while (records.hasNext()) &#123;</span><br><span class="line">        insertRecordIntoSorter(records.next());</span><br><span class="line">      &#125;</span><br><span class="line">      closeAndWriteOutput();</span><br><span class="line">      success = true;</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      if (sorter != null) &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">          sorter.cleanupResources();</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">          // Only throw this error if we won&#x27;t be masking another</span><br><span class="line">          // error.</span><br><span class="line">          if (success) &#123;</span><br><span class="line">            throw e;</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            logger.error(&quot;In addition to a failure during writing, we failed during &quot; +</span><br><span class="line">                         &quot;cleanup.&quot;, e);</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在上面的代码中，会先对每个记录执行insertRecordIntoSorter方法，下面看一下这个方法的源码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\UnsafeShuffleWriter.java</span><br><span class="line">  void insertRecordIntoSorter(Product2&lt;K, V&gt; record) throws IOException &#123;</span><br><span class="line">    assert(sorter != null);</span><br><span class="line">    final K key = record._1();</span><br><span class="line">    final int partitionId = partitioner.getPartition(key);</span><br><span class="line">    serBuffer.reset();</span><br><span class="line">    serOutputStream.writeKey(key, OBJECT_CLASS_TAG);</span><br><span class="line">    serOutputStream.writeValue(record._2(), OBJECT_CLASS_TAG);</span><br><span class="line">    serOutputStream.flush();</span><br><span class="line"></span><br><span class="line">    final int serializedRecordSize = serBuffer.size();</span><br><span class="line">    assert (serializedRecordSize &gt; 0);</span><br><span class="line"></span><br><span class="line">    sorter.insertRecord(</span><br><span class="line">      serBuffer.getBuf(), Platform.BYTE_ARRAY_OFFSET, serializedRecordSize, partitionId);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在上面的代码中，最重要的是最后一行把序列化的数据插入到ShuffleExternalSorter的实例sorter中，然后继续跟踪这个代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\ShuffleExternalSorter.java</span><br><span class="line">  public void insertRecord(Object recordBase, long recordOffset, int length, int partitionId)</span><br><span class="line">    throws IOException &#123;</span><br><span class="line"></span><br><span class="line">    // for tests</span><br><span class="line">    assert(inMemSorter != null);</span><br><span class="line">    if (inMemSorter.numRecords() &gt;= numElementsForSpillThreshold) &#123;</span><br><span class="line">      logger.info(&quot;Spilling data because number of spilledRecords crossed the threshold &quot; +</span><br><span class="line">        numElementsForSpillThreshold);</span><br><span class="line">      spill();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    growPointerArrayIfNecessary();</span><br><span class="line">    // Need 4 bytes to store the record length.</span><br><span class="line">    final int required = length + 4;</span><br><span class="line">    acquireNewPageIfNecessary(required);</span><br><span class="line"></span><br><span class="line">    assert(currentPage != null);</span><br><span class="line">    final Object base = currentPage.getBaseObject();</span><br><span class="line">    final long recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);</span><br><span class="line">    Platform.putInt(base, pageCursor, length);</span><br><span class="line">    pageCursor += 4;</span><br><span class="line">    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);</span><br><span class="line">    pageCursor += length;</span><br><span class="line">    inMemSorter.insertRecord(recordAddress, partitionId);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>上面的代码中，首先是判断是否需要spill到磁盘中，这个部分内容较多，一会儿再仔细的说。先假设目前还不需要溢写磁盘，继续往下看。</p>
<p>首先是判断一下是否需要为指针数组(这个数组一会儿介绍)申请额外的内存，然后是看一下是否需要申请额外的内存页，再接下来是产生recordAddress，先深入进入，看一下这个地址是怎么产生：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\memory\TaskMemoryManager.java</span><br><span class="line">  public long encodePageNumberAndOffset(MemoryBlock page, long offsetInPage) &#123;</span><br><span class="line">    if (tungstenMemoryMode == MemoryMode.OFF_HEAP) &#123;</span><br><span class="line">      // In off-heap mode, an offset is an absolute address that may require a full 64 bits to</span><br><span class="line">      // encode. Due to our page size limitation, though, we can convert this into an offset that&#x27;s</span><br><span class="line">      // relative to the page&#x27;s base offset; this relative offset will fit in 51 bits.</span><br><span class="line">      offsetInPage -= page.getBaseOffset();</span><br><span class="line">    &#125;</span><br><span class="line">    return encodePageNumberAndOffset(page.pageNumber, offsetInPage);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static long encodePageNumberAndOffset(int pageNumber, long offsetInPage) &#123;</span><br><span class="line">    assert (pageNumber != -1) : &quot;encodePageNumberAndOffset called with invalid page&quot;;</span><br><span class="line">    return (((long) pageNumber) &lt;&lt; OFFSET_BITS) | (offsetInPage &amp; MASK_LONG_LOWER_51_BITS);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，对于使用堆内和堆外是一点区别的，使用堆外的话，由于当前的offsetInPage已经是一个64位的值了，需要跟base值做一个减法，这样可以得到比较小的相对值，而对于使用堆内内存的情况来说，offsetInPage已经是基于base的相对值了，不必考虑这个问题。</p>
<p>紧接着调用的encodePageNumberAndOffset方法里面，把pageNumber左移了51位，offset只保留了低的51位，这样它们拼在了一起。</p>
<p><strong>[13 bit memory page number][51 bit offset in page]</strong></p>
<p>不过，这个不是最终是结果，因为一会儿还要把partition id引入进来。</p>
<p>现在，回到insertRecord函数里面继续往下看，在得到record的地址的起点之后，先把序列化的数据的length放入Platform中，由于是int型的，所以游标+4，紧接着，把序列化的数据放入Platform中，由于已经知道长度是length，所以游标加length。这样就完成了整条记录放到了内存之中。最后一行很重要inMemSorter.insertRecord(recordAddress, partitionId)，继续深入到这个函数里面，这个函数的位置是：&#x2F;Users&#x2F;yakun&#x2F;workspace&#x2F;leap_git_code&#x2F;spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;spark&#x2F;shuffle&#x2F;sort&#x2F;ShuffleInMemorySorter.java</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\ShuffleInMemorySorter.java</span><br><span class="line">  public void insertRecord(long recordPointer, int partitionId) &#123;</span><br><span class="line">    if (!hasSpaceForAnotherRecord()) &#123;</span><br><span class="line">      throw new IllegalStateException(&quot;There is no space for new record&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    array.set(pos, PackedRecordPointer.packPointer(recordPointer, partitionId));</span><br><span class="line">    pos++;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这个函数最主要的功能是，把recored数据在内存中记录的起点和partitionid处理一下，生成一个PackedRecordPointer，然后把这个对象放到数组array里面。下面看一下PackedRecordPointer.packPointer方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\PackedRecordPointer.java</span><br><span class="line">  public static long packPointer(long recordPointer, int partitionId) &#123;</span><br><span class="line">    assert (partitionId &lt;= MAXIMUM_PARTITION_ID);</span><br><span class="line">    // Note that without word alignment we can address 2^27 bytes = 128 megabytes per page.</span><br><span class="line">    // Also note that this relies on some internals of how TaskMemoryManager encodes its addresses.</span><br><span class="line">    final long pageNumber = (recordPointer &amp; MASK_LONG_UPPER_13_BITS) &gt;&gt;&gt; 24;</span><br><span class="line">    final long compressedAddress = pageNumber | (recordPointer &amp; MASK_LONG_LOWER_27_BITS);</span><br><span class="line">    return (((long) partitionId) &lt;&lt; 40) | compressedAddress;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这块代码，首先只保留recordPointer的高13位(pagenumber)有值，然后整体右移动24位，这样得到了[24位0][13位pagenumber][27位0]的新pagenumber；然后recordPointer的低27位，并跟刚才的结果取或，得到了[24位0][13位pagenumber][27位page内offset];最后，再把partitionid左移40位，跟刚才的结果取或，得到了最终的结果是：</p>
<p><strong>[24位partition id][13位pagenumber][27位page内offset]</strong></p>
<p>然后这个对象会放到数组array里面。</p>
<p>以上就是每个记录的写入过程了，在回到前面介绍溢写之前，先讨论一下这个内存管理器。</p>
<p>page的总容量为2^13,每个page的寻址范围是2^27，因此总的寻址范围是2^40&#x3D;1T，注意这相当于是虚拟内存的寻址，相当于是一共有1T个指针，每个指针指向了一个真实的物理地址，由于64位系统的内存都是8字节对齐的，也就是说一个指针就能指向8个字节，也就是说，对于整体的内存的使用能力为1T*8B&#x3D;8TB。</p>
<p>还有一点非常奇思妙想，就是在实现内存页管理的基础之上，直接在逻辑地址的前24位写上了对应的partitionid，这是为了，未来在对record进行排序时，无需进行反序列化，直接拿这个地址就可以进行比较。</p>
<p>下面再回到insertRecord函数里面，研究一下spill(溢写磁盘)。在insertRecord里面有个判断，如果record的数量大于一个配置值spark.shuffle.spill.numElementsForceSpillThreshold(默认是1G个)，就调用spill()方法，下面进入spill()方法内部，这个方法首先是调用了自己的父类的spill，然后父类调用重载函数spill(Long.MAX_VALUE, this)，由于刚刚的子类已经覆盖了这个方法，就又回到了刚才的类里面，具体的代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\ShuffleExternalSorter.java</span><br><span class="line">  public long spill(long size, MemoryConsumer trigger) throws IOException &#123;</span><br><span class="line">    if (trigger != this || inMemSorter == null || inMemSorter.numRecords() == 0) &#123;</span><br><span class="line">      return 0L;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    logger.info(&quot;Thread &#123;&#125; spilling sort data of &#123;&#125; to disk (&#123;&#125; &#123;&#125; so far)&quot;,</span><br><span class="line">      Thread.currentThread().getId(),</span><br><span class="line">      Utils.bytesToString(getMemoryUsage()),</span><br><span class="line">      spills.size(),</span><br><span class="line">      spills.size() &gt; 1 ? &quot; times&quot; : &quot; time&quot;);</span><br><span class="line"></span><br><span class="line">    writeSortedFile(false);</span><br><span class="line">    final long spillSize = freeMemory();</span><br><span class="line">    inMemSorter.reset();</span><br><span class="line">    // Reset the in-memory sorter&#x27;s pointer array only after freeing up the memory pages holding the</span><br><span class="line">    // records. Otherwise, if the task is over allocated memory, then without freeing the memory</span><br><span class="line">    // pages, we might not be able to get memory for the pointer array.</span><br><span class="line">    taskContext.taskMetrics().incMemoryBytesSpilled(spillSize);</span><br><span class="line">    return spillSize;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这个函数最重要的功能就是调用一下writeSortedFile，然后清理一下现场。下面直接讨论一下writeSortedFile函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\ShuffleExternalSorter.java</span><br><span class="line">  private void writeSortedFile(boolean isLastFile) throws IOException &#123;</span><br><span class="line"></span><br><span class="line">    final ShuffleWriteMetrics writeMetricsToUse;</span><br><span class="line"></span><br><span class="line">    if (isLastFile) &#123;</span><br><span class="line">      // We&#x27;re writing the final non-spill file, so we _do_ want to count this as shuffle bytes.</span><br><span class="line">      writeMetricsToUse = writeMetrics;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      // We&#x27;re spilling, so bytes written should be counted towards spill rather than write.</span><br><span class="line">      // Create a dummy WriteMetrics object to absorb these metrics, since we don&#x27;t want to count</span><br><span class="line">      // them towards shuffle bytes written.</span><br><span class="line">      writeMetricsToUse = new ShuffleWriteMetrics();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // This call performs the actual sort.</span><br><span class="line">    final ShuffleInMemorySorter.ShuffleSorterIterator sortedRecords =</span><br><span class="line">      inMemSorter.getSortedIterator();</span><br><span class="line"></span><br><span class="line">    // Currently, we need to open a new DiskBlockObjectWriter for each partition; we can avoid this</span><br><span class="line">    // after SPARK-5581 is fixed.</span><br><span class="line">    DiskBlockObjectWriter writer;</span><br><span class="line"></span><br><span class="line">    // Small writes to DiskBlockObjectWriter will be fairly inefficient. Since there doesn&#x27;t seem to</span><br><span class="line">    // be an API to directly transfer bytes from managed memory to the disk writer, we buffer</span><br><span class="line">    // data through a byte array. This array does not need to be large enough to hold a single</span><br><span class="line">    // record;</span><br><span class="line">    final byte[] writeBuffer = new byte[DISK_WRITE_BUFFER_SIZE];</span><br><span class="line"></span><br><span class="line">    // Because this output will be read during shuffle, its compression codec must be controlled by</span><br><span class="line">    // spark.shuffle.compress instead of spark.shuffle.spill.compress, so we need to use</span><br><span class="line">    // createTempShuffleBlock here; see SPARK-3426 for more details.</span><br><span class="line">    final Tuple2&lt;TempShuffleBlockId, File&gt; spilledFileInfo =</span><br><span class="line">      blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">    final File file = spilledFileInfo._2();</span><br><span class="line">    final TempShuffleBlockId blockId = spilledFileInfo._1();</span><br><span class="line">    final SpillInfo spillInfo = new SpillInfo(numPartitions, file, blockId);</span><br><span class="line"></span><br><span class="line">    // Unfortunately, we need a serializer instance in order to construct a DiskBlockObjectWriter.</span><br><span class="line">    // Our write path doesn&#x27;t actually use this serializer (since we end up calling the `write()`</span><br><span class="line">    // OutputStream methods), but DiskBlockObjectWriter still calls some methods on it. To work</span><br><span class="line">    // around this, we pass a dummy no-op serializer.</span><br><span class="line">    final SerializerInstance ser = DummySerializerInstance.INSTANCE;</span><br><span class="line"></span><br><span class="line">    writer = blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse);</span><br><span class="line"></span><br><span class="line">    int currentPartition = -1;</span><br><span class="line">    while (sortedRecords.hasNext()) &#123;</span><br><span class="line">      sortedRecords.loadNext();</span><br><span class="line">      final int partition = sortedRecords.packedRecordPointer.getPartitionId();</span><br><span class="line">      assert (partition &gt;= currentPartition);</span><br><span class="line">      if (partition != currentPartition) &#123;</span><br><span class="line">        // Switch to the new partition</span><br><span class="line">        if (currentPartition != -1) &#123;</span><br><span class="line">          writer.commitAndClose();</span><br><span class="line">          spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();</span><br><span class="line">        &#125;</span><br><span class="line">        currentPartition = partition;</span><br><span class="line">        writer =</span><br><span class="line">          blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();</span><br><span class="line">      final Object recordPage = taskMemoryManager.getPage(recordPointer);</span><br><span class="line">      final long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);</span><br><span class="line">      int dataRemaining = Platform.getInt(recordPage, recordOffsetInPage);</span><br><span class="line">      long recordReadPosition = recordOffsetInPage + 4; // skip over record length</span><br><span class="line">      while (dataRemaining &gt; 0) &#123;</span><br><span class="line">        final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining);</span><br><span class="line">        Platform.copyMemory(</span><br><span class="line">          recordPage, recordReadPosition, writeBuffer, Platform.BYTE_ARRAY_OFFSET, toTransfer);</span><br><span class="line">        writer.write(writeBuffer, 0, toTransfer);</span><br><span class="line">        recordReadPosition += toTransfer;</span><br><span class="line">        dataRemaining -= toTransfer;</span><br><span class="line">      &#125;</span><br><span class="line">      writer.recordWritten();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (writer != null) &#123;</span><br><span class="line">      writer.commitAndClose();</span><br><span class="line">      // If `writeSortedFile()` was called from `closeAndGetSpills()` and no records were inserted,</span><br><span class="line">      // then the file might be empty. Note that it might be better to avoid calling</span><br><span class="line">      // writeSortedFile() in that case.</span><br><span class="line">      if (currentPartition != -1) &#123;</span><br><span class="line">        spillInfo.partitionLengths[currentPartition] = writer.fileSegment().length();</span><br><span class="line">        spills.add(spillInfo);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if (!isLastFile) &#123;  // i.e. this is a spill file</span><br><span class="line">      // The current semantics of `shuffleRecordsWritten` seem to be that it&#x27;s updated when records</span><br><span class="line">      // are written to disk, not when they enter the shuffle sorting code. DiskBlockObjectWriter</span><br><span class="line">      // relies on its `recordWritten()` method being called in order to trigger periodic updates to</span><br><span class="line">      // `shuffleBytesWritten`. If we were to remove the `recordWritten()` call and increment that</span><br><span class="line">      // counter at a higher-level, then the in-progress metrics for records written and bytes</span><br><span class="line">      // written would get out of sync.</span><br><span class="line">      //</span><br><span class="line">      // When writing the last file, we pass `writeMetrics` directly to the DiskBlockObjectWriter;</span><br><span class="line">      // in all other cases, we pass in a dummy write metrics to capture metrics, then copy those</span><br><span class="line">      // metrics to the true write metrics here. The reason for performing this copying is so that</span><br><span class="line">      // we can avoid reporting spilled bytes as shuffle write bytes.</span><br><span class="line">      //</span><br><span class="line">      // Note that we intentionally ignore the value of `writeMetricsToUse.shuffleWriteTime()`.</span><br><span class="line">      // Consistent with ExternalSorter, we do not count this IO towards shuffle write time.</span><br><span class="line">      // This means that this IO time is not accounted for anywhere; SPARK-3577 will fix this.</span><br><span class="line">      writeMetrics.incRecordsWritten(writeMetricsToUse.recordsWritten());</span><br><span class="line">      taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.bytesWritten());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这个函数接收一个参数代表是否是最后一次写，因为前面都是满足条数了写一次，正常来说，执行到最后的时候，会剩下一点在内存里面，当要把最后剩下的这点内存数据写入文件时，此参数为true，不过此参数影响不大，只是影响一些实时指标统计。下面这一行非常重要：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// This call performs the actual sort.</span><br><span class="line">final ShuffleInMemorySorter.ShuffleSorterIterator sortedRecords =</span><br><span class="line">  inMemSorter.getSortedIterator();</span><br></pre></td></tr></table></figure>

<p>这一行真正的发生了sort，下面跟进一下，看一下getSortedIterator的源码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\ShuffleInMemorySorter.java</span><br><span class="line">  public ShuffleSorterIterator getSortedIterator() &#123;</span><br><span class="line">    int offset = 0;</span><br><span class="line">    if (useRadixSort) &#123;</span><br><span class="line">      offset = RadixSort.sort(</span><br><span class="line">        array, pos,</span><br><span class="line">        PackedRecordPointer.PARTITION_ID_START_BYTE_INDEX,</span><br><span class="line">        PackedRecordPointer.PARTITION_ID_END_BYTE_INDEX, false, false);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      MemoryBlock unused = new MemoryBlock(</span><br><span class="line">        array.getBaseObject(),</span><br><span class="line">        array.getBaseOffset() + pos * 8L,</span><br><span class="line">        (array.size() - pos) * 8L);</span><br><span class="line">      LongArray buffer = new LongArray(unused);</span><br><span class="line">      Sorter&lt;PackedRecordPointer, LongArray&gt; sorter =</span><br><span class="line">        new Sorter&lt;&gt;(new ShuffleSortDataFormat(buffer));</span><br><span class="line"></span><br><span class="line">      sorter.sort(array, 0, pos, SORT_COMPARATOR);</span><br><span class="line">    &#125;</span><br><span class="line">    return new ShuffleSorterIterator(pos, array, offset);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这段源码里面会有两种排序算法，但是都是使用partition来进行排序，不再继续深入，返回到writeSortedFile函数里面。后面的逻辑就是把已经按照partition排好序的文件写入到真正的block里面。</p>
<p>以上就是遍历所有记录写入到文件的过程了，然后再回到一开始的入口函数：UnsafeShuffleWriter的write方法，继续往下看：closeAndWriteOutput()，再进入这个方法里面看一下源码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\UnsafeShuffleWriter.java</span><br><span class="line">  void closeAndWriteOutput() throws IOException &#123;</span><br><span class="line">    assert(sorter != null);</span><br><span class="line">    updatePeakMemoryUsed();</span><br><span class="line">    serBuffer = null;</span><br><span class="line">    serOutputStream = null;</span><br><span class="line">    final SpillInfo[] spills = sorter.closeAndGetSpills();</span><br><span class="line">    sorter = null;</span><br><span class="line">    final long[] partitionLengths;</span><br><span class="line">    final File output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">    final File tmp = Utils.tempFileWith(output);</span><br><span class="line">    try &#123;</span><br><span class="line">      partitionLengths = mergeSpills(spills, tmp);</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      for (SpillInfo spill : spills) &#123;</span><br><span class="line">        if (spill.file.exists() &amp;&amp; ! spill.file.delete()) &#123;</span><br><span class="line">          logger.error(&quot;Error while deleting spill file &#123;&#125;&quot;, spill.file.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，与其它的ShuffleHandle类似的处理，进行归并，然后写一个结果文件和一个index文件，完成了mapStatus.</p>
<h4 id="6-2-1-3-BypassMergeSortShuffleHandle"><a href="#6-2-1-3-BypassMergeSortShuffleHandle" class="headerlink" title="6.2.1.3 BypassMergeSortShuffleHandle"></a>6.2.1.3 BypassMergeSortShuffleHandle</h4><p>如果使用的是BypassMergeSortShuffleHandle，它的写操作是什么样子的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\java\org\apache\spark\shuffle\sort\BypassMergeSortShuffleWriter.java</span><br><span class="line">  public void write(Iterator&lt;Product2&lt;K, V&gt;&gt; records) throws IOException &#123;</span><br><span class="line">    assert (partitionWriters == null);</span><br><span class="line">    if (!records.hasNext()) &#123;</span><br><span class="line">      partitionLengths = new long[numPartitions];</span><br><span class="line">      shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, null);</span><br><span class="line">      mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">      return;</span><br><span class="line">    &#125;</span><br><span class="line">    final SerializerInstance serInstance = serializer.newInstance();</span><br><span class="line">    final long openStartTime = System.nanoTime();</span><br><span class="line">    partitionWriters = new DiskBlockObjectWriter[numPartitions];</span><br><span class="line">    partitionWriterSegments = new FileSegment[numPartitions];</span><br><span class="line">    for (int i = 0; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      final Tuple2&lt;TempShuffleBlockId, File&gt; tempShuffleBlockIdPlusFile =</span><br><span class="line">        blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">      final File file = tempShuffleBlockIdPlusFile._2();</span><br><span class="line">      final BlockId blockId = tempShuffleBlockIdPlusFile._1();</span><br><span class="line">      partitionWriters[i] =</span><br><span class="line">        blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">    &#125;</span><br><span class="line">    // Creating the file to write to and creating a disk writer both involve interacting with</span><br><span class="line">    // the disk, and can take a long time in aggregate when we open many files, so should be</span><br><span class="line">    // included in the shuffle write time.</span><br><span class="line">    writeMetrics.incWriteTime(System.nanoTime() - openStartTime);</span><br><span class="line"></span><br><span class="line">    while (records.hasNext()) &#123;</span><br><span class="line">      final Product2&lt;K, V&gt; record = records.next();</span><br><span class="line">      final K key = record._1();</span><br><span class="line">      partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for (int i = 0; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      final DiskBlockObjectWriter writer = partitionWriters[i];</span><br><span class="line">      partitionWriterSegments[i] = writer.commitAndGet();</span><br><span class="line">      writer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    File output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">    File tmp = Utils.tempFileWith(output);</span><br><span class="line">    try &#123;</span><br><span class="line">      partitionLengths = writePartitionedFile(tmp);</span><br><span class="line">      shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">    &#125; finally &#123;</span><br><span class="line">      if (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">        logger.error(&quot;Error while deleting temp file &#123;&#125;&quot;, tmp.getAbsolutePath());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>根据numPartitions来创建对应数目的DiskBlockObjectWriter，即写临时文件的handler,每个handler需要32KB 的Buffer。<br>主要过程：遍历该map任务的分区数据，然后通过partitioner.getPartition(key)确定该record写入到那个临时文件，然后通过DiskBlockObjectWriter写入到临时文件。数据都写入临时文件之后，再把所有临时文件归并为一个最终的文件和其对应的索引文件。</p>
<h3 id="6-2-2-将结果通知Driver"><a href="#6-2-2-将结果通知Driver" class="headerlink" title="6.2.2 将结果通知Driver"></a>6.2.2 将结果通知Driver</h3><p>下面继续回到TaskRunner类里面，在task自身的run方法执行结束后，会清理一下现场，接下来就是如果处理刚刚得到的结果了。</p>
<p>先把结果序列化一下，然后看一下结果的大小是否超过了最大值(可以通过spark.driver.maxResultSize来配置)，如果超过了，会转成IndirectTaskResult类型的结果，否则，看一下结果的大小是否超过了maxDirectResultSize(可以通过spark.task.maxDirectResultSize来配置)，如果超过了就存储在blockManager中，并将存储的位置和大小返回成IndirectTaskResult类型的结果；如果以上两者都是小于等于的关系，那么会直接返回结果，代码片段如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark\spark-branch-2.1.0\core\src\main\scala\org\apache\spark\executor\Executor.scala</span><br><span class="line">        val serializedResult: ByteBuffer = &#123;</span><br><span class="line">          if (maxResultSize &gt; 0 &amp;&amp; resultSize &gt; maxResultSize) &#123;</span><br><span class="line">            logWarning(s&quot;Finished $taskName (TID $taskId). Result is larger than maxResultSize &quot; +</span><br><span class="line">              s&quot;($&#123;Utils.bytesToString(resultSize)&#125; &gt; $&#123;Utils.bytesToString(maxResultSize)&#125;), &quot; +</span><br><span class="line">              s&quot;dropping it.&quot;)</span><br><span class="line">            ser.serialize(new IndirectTaskResult[Any](TaskResultBlockId(taskId), resultSize))</span><br><span class="line">          &#125; else if (resultSize &gt; maxDirectResultSize) &#123;</span><br><span class="line">            val blockId = TaskResultBlockId(taskId)</span><br><span class="line">            env.blockManager.putBytes(</span><br><span class="line">              blockId,</span><br><span class="line">              new ChunkedByteBuffer(serializedDirectResult.duplicate()),</span><br><span class="line">              StorageLevel.MEMORY_AND_DISK_SER)</span><br><span class="line">            logInfo(</span><br><span class="line">              s&quot;Finished $taskName (TID $taskId). $resultSize bytes result sent via BlockManager)&quot;)</span><br><span class="line">            ser.serialize(new IndirectTaskResult[Any](blockId, resultSize))</span><br><span class="line">          &#125; else &#123;</span><br><span class="line">            logInfo(s&quot;Finished $taskName (TID $taskId). $resultSize bytes result sent to driver&quot;)</span><br><span class="line">            serializedDirectResult</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)</span><br></pre></td></tr></table></figure>

<p>最终，通过execBackend.statusUpdate来告知driver计算结束了，并给出计算结果的信息。告知的方式是通过driverRef.send(msg)去发送一个消息，与之前Driver发送Task的方式类似，源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/executor/CoarseGrainedExecutorBackend.scala</span><br><span class="line">  override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) &#123;</span><br><span class="line">    val msg = StatusUpdate(executorId, taskId, state, data)</span><br><span class="line">    driver match &#123;</span><br><span class="line">      case Some(driverRef) =&gt; driverRef.send(msg)</span><br><span class="line">      case None =&gt; logWarning(s&quot;Drop $msg because has not yet connected to driver&quot;)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="6-3-Driver处理结果"><a href="#6-3-Driver处理结果" class="headerlink" title="6.3 Driver处理结果"></a>6.3 Driver处理结果</h2><p>在Executor运行完一个Task后，会给Driver发送消息StatusUpdate类型的消息，然后回到Driver的源码继续这个流程，在spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;cluster&#x2F;CoarseGrainedSchedulerBackend.scala中会收到这个消息，处理的逻辑在这个CoarseGrainedSchedulerBackend这个类的receive方法中，源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala</span><br><span class="line">      case StatusUpdate(executorId, taskId, state, data) =&gt;</span><br><span class="line">        scheduler.statusUpdate(taskId, state, data.value)</span><br><span class="line">        if (TaskState.isFinished(state)) &#123;</span><br><span class="line">          executorDataMap.get(executorId) match &#123;</span><br><span class="line">            case Some(executorInfo) =&gt;</span><br><span class="line">              executorInfo.freeCores += scheduler.CPUS_PER_TASK</span><br><span class="line">              makeOffers(executorId)</span><br><span class="line">            case None =&gt;</span><br><span class="line">              // Ignoring the update since we don&#x27;t know about the executor.</span><br><span class="line">              logWarning(s&quot;Ignored task status update ($taskId state $state) &quot; +</span><br><span class="line">                s&quot;from unknown executor with ID $executorId&quot;)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>处理的逻辑是，调用scheduler.statusUpdate(taskId, state, data.value)。</p>
<p>然后跳转进入到statusUpdate这个方法内部，最主要的逻辑是，从这个Task的TaskSetManager内部除去这个TaskID，然后把记录的对应excutor中的task数量减一，再然后，在taskSet里面删除这个运行中的task，再然后，调用了taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)方法来处理结果。</p>
<p>enqueueSuccessfulTask在类TaskResultGetter中，源码位置是：spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;scheduler&#x2F;TaskResultGetter.scala</p>
<ul>
<li>如果result是directResult，那么直接取出结果；</li>
<li>如果是IndirectTaskResult，那么需要调用 blockManager.getRemoteBytes() 去 fech 实际的 result。在获得了result之后，就会更新一下当前收到的所有结果的大小。再然后，就去调用scheduler.handleSuccessfulTask(taskSetManager, tid, result)方法来进行下一步的处理。</li>
</ul>
<p>然后handleSuccessfulTask里面调用了sched.dagScheduler.taskEnded(tasks(index), Success, result.value(), result.accumUpdates, info)来处理，再进入到DAGScheduler中的taskEnded方法，这里向eventProcessLoop传递了一个CompletionEvent事件，源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala</span><br><span class="line">  def taskEnded(</span><br><span class="line">      task: Task[_],</span><br><span class="line">      reason: TaskEndReason,</span><br><span class="line">      result: Any,</span><br><span class="line">      accumUpdates: Seq[AccumulatorV2[_, _]],</span><br><span class="line">      taskInfo: TaskInfo): Unit = &#123;</span><br><span class="line">    eventProcessLoop.post(</span><br><span class="line">      CompletionEvent(task, reason, result, accumUpdates, taskInfo))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>然后再回到一开始处理JobSubmitted事件的地方，进入到DAGSchedulerEventProcessLoop类里面的doOnReceive方法里面，看到了对CompletionEvent事件的处理，即dagScheduler.handleTaskCompletion(completion)，然后再转战到handleTaskCompletion方法里面，看到对于event的reason为Success的类型中的task的类型不同是有不同的处理逻辑的：</p>
<ul>
<li><p>对于ResultTask类型的Task，会通过updateAccumulators(event)来累加的计算各个结果，如果完成的task的数量已经达到这个Stage的Partition的数量，那么，标记这个Stage为完成状态。由于这个Task的类型是ResultTask，那也就代表着，这个Stage完成之后，整个Job完成了。</p>
</li>
<li><p>对于ShuffleMapTask类型的Task，先记录一下输出结果的位置。如果正在运行中的stage列表包含此stage，并且此stage已经没有等待处理的Partition了，那么标记此Stage为成功结束，然后在mapOutputTracker中记录刚刚的输出结果，以后于后续的shuffle read的时候读取。</p>
</li>
</ul>
<p>对于两种类型的Task都会调用submitWaitingStages()方法来提交剩下的其它的Stage，但是在ResultStage结束后，已经没有在Waiting的Stage了，程序也就正常运行结束；在ShuffleMapStage运行结束后，会通过submitWaitingStages()方法来提交接下来的Stage。submitWaitingStages()方法内部逻辑非常简单，就是遍历一下当前等待执行的Stage列表，然后调用一开始时已经讲过的submitStage(stage)方法，代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">private def submitWaitingStages() &#123;</span><br><span class="line">  // TODO: We might want to run this less often, when we are sure that something has become</span><br><span class="line">  // runnable that wasn&#x27;t before.</span><br><span class="line">  logTrace(&quot;Checking for newly runnable parent stages&quot;)</span><br><span class="line">  logTrace(&quot;running: &quot; + runningStages)</span><br><span class="line">  logTrace(&quot;waiting: &quot; + waitingStages)</span><br><span class="line">  logTrace(&quot;failed: &quot; + failedStages)</span><br><span class="line">  val waitingStagesCopy = waitingStages.toArray</span><br><span class="line">  waitingStages.clear()</span><br><span class="line">  for (stage &lt;- waitingStagesCopy.sortBy(_.firstJobId)) &#123;</span><br><span class="line">    submitStage(stage)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="6-4-ShuffleRead"><a href="#6-4-ShuffleRead" class="headerlink" title="6.4 ShuffleRead"></a>6.4 ShuffleRead</h2><p>通过上面的流程分析，已经能够得到整个Spark任务的计算过程是：Driver创建TaskSet，然后交给Executor去执行，之后的执行结果再告知Driver。这个流程已经能够完成一个基本的Stage了，但是，Spark中的Stage划分是以Shuffle为基础的，也就是说，一个非常常见的情况是，后面的Stage很可能是需要前面的Stage的输出结果来作为输入的。</p>
<p>下面再讨论一下，如何进行ShuffleRead。</p>
<p>reducer 首先要知道 parent stage 中 ShuffleMapTask 输出的 FileSegments 在哪个节点。这个信息在 ShuffleMapTask 完成时已经送到了 driver 的 mapOutputTrackerMaster，并存放到了 mapStatuses: HashMap 里面，给定 stageId，可以获取该 stage 中 ShuffleMapTasks 生成的 FileSegments 信息 Array[MapStatus]，通过 Array(taskId) 就可以得到某个 task 输出的 FileSegments 位置（blockManagerId）及每个 FileSegment 大小。</p>
<p>整个故事的起点要从ShuffledRDD开始，这个类位于：spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;rdd&#x2F;ShuffledRDD.scala</p>
<p>这个类中的compute方法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala</span><br><span class="line">  override def compute(split: Partition, context: TaskContext): Iterator[(K, C)] = &#123;</span><br><span class="line">    val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]</span><br><span class="line">    SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)</span><br><span class="line">      .read()</span><br><span class="line">      .asInstanceOf[Iterator[(K, C)]]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在上面的方法中的shuffleManager是一个SortShuffleManager的实例(目前的Spark中只有这一种)，然后在SortShuffleManager类里面，调用的getReader方法的源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala</span><br><span class="line">  override def getReader[K, C](</span><br><span class="line">      handle: ShuffleHandle,</span><br><span class="line">      startPartition: Int,</span><br><span class="line">      endPartition: Int,</span><br><span class="line">      context: TaskContext): ShuffleReader[K, C] = &#123;</span><br><span class="line">    new BlockStoreShuffleReader(</span><br><span class="line">      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], startPartition, endPartition, context)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>然后，刚刚的代码中的read()方法，自然是会调用BlockStoreShuffleReader中的read()方法，这个类的位置是：spark&#x2F;spark-branch-2.0&#x2F;core&#x2F;src&#x2F;main&#x2F;scala&#x2F;org&#x2F;apache&#x2F;spark&#x2F;shuffle&#x2F;BlockStoreShuffleReader.scala</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">//位置：spark/spark-branch-2.0/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala</span><br><span class="line">  override def read(): Iterator[Product2[K, C]] = &#123;</span><br><span class="line">    val blockFetcherItr = new ShuffleBlockFetcherIterator(</span><br><span class="line">      context,</span><br><span class="line">      blockManager.shuffleClient,</span><br><span class="line">      blockManager,</span><br><span class="line">      mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class="line">      // Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span><br><span class="line">      SparkEnv.get.conf.getSizeAsMb(&quot;spark.reducer.maxSizeInFlight&quot;, &quot;48m&quot;) * 1024 * 1024,</span><br><span class="line">      SparkEnv.get.conf.getInt(&quot;spark.reducer.maxReqsInFlight&quot;, Int.MaxValue))</span><br><span class="line"></span><br><span class="line">    // Wrap the streams for compression based on configuration</span><br><span class="line">    val wrappedStreams = blockFetcherItr.map &#123; case (blockId, inputStream) =&gt;</span><br><span class="line">      serializerManager.wrapForCompression(blockId, inputStream)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val serializerInstance = dep.serializer.newInstance()</span><br><span class="line"></span><br><span class="line">    // Create a key/value iterator for each stream</span><br><span class="line">    val recordIter = wrappedStreams.flatMap &#123; wrappedStream =&gt;</span><br><span class="line">      // Note: the asKeyValueIterator below wraps a key/value iterator inside of a</span><br><span class="line">      // NextIterator. The NextIterator makes sure that close() is called on the</span><br><span class="line">      // underlying InputStream when all records have been read.</span><br><span class="line">      serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Update the context task metrics for each record read.</span><br><span class="line">    val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class="line">    val metricIter = CompletionIterator[(Any, Any), Iterator[(Any, Any)]](</span><br><span class="line">      recordIter.map &#123; record =&gt;</span><br><span class="line">        readMetrics.incRecordsRead(1)</span><br><span class="line">        record</span><br><span class="line">      &#125;,</span><br><span class="line">      context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class="line"></span><br><span class="line">    // An interruptible iterator must be used here in order to support task cancellation</span><br><span class="line">    val interruptibleIter = new InterruptibleIterator[(Any, Any)](context, metricIter)</span><br><span class="line"></span><br><span class="line">    val aggregatedIter: Iterator[Product2[K, C]] = if (dep.aggregator.isDefined) &#123;</span><br><span class="line">      if (dep.mapSideCombine) &#123;</span><br><span class="line">        // We are reading values that are already combined</span><br><span class="line">        val combinedKeyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, C)]]</span><br><span class="line">        dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        // We don&#x27;t know the value type, but also don&#x27;t care -- the dependency *should*</span><br><span class="line">        // have made sure its compatible w/ this aggregator, which will convert the value</span><br><span class="line">        // type to the combined type C</span><br><span class="line">        val keyValuesIterator = interruptibleIter.asInstanceOf[Iterator[(K, Nothing)]]</span><br><span class="line">        dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      require(!dep.mapSideCombine, &quot;Map-side combine without Aggregator specified!&quot;)</span><br><span class="line">      interruptibleIter.asInstanceOf[Iterator[Product2[K, C]]]</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Sort the output if there is a sort ordering defined.</span><br><span class="line">    dep.keyOrdering match &#123;</span><br><span class="line">      case Some(keyOrd: Ordering[K]) =&gt;</span><br><span class="line">        // Create an ExternalSorter to sort the data. Note that if spark.shuffle.spill is disabled,</span><br><span class="line">        // the ExternalSorter won&#x27;t spill to disk.</span><br><span class="line">        val sorter =</span><br><span class="line">          new ExternalSorter[K, C, C](context, ordering = Some(keyOrd), serializer = dep.serializer)</span><br><span class="line">        sorter.insertAll(aggregatedIter)</span><br><span class="line">        context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class="line">        context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class="line">        context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class="line">        CompletionIterator[Product2[K, C], Iterator[Product2[K, C]]](sorter.iterator, sorter.stop())</span><br><span class="line">      case None =&gt;</span><br><span class="line">        aggregatedIter</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在这个read()方法里面，首先会创建一个ShuffleBlockFetcherIterator的实例对象，然后进入ShuffleBlockFetcherIterator，发现它会在构造时，调用initialize方法，在这个initialize方法中，首先会调用splitLocalRemoteBlocks方法来把所有请求分成远程和本地两种类型，以及明确一共要获取多少个块，然后通过方法fetchUpToMaxBytes()来不间断(这里有两个参数可以控制速率，分别的：spark.reducer.maxSizeInFlight和spark.reducer.maxReqsInFlight”)的获取所有远程的文件块。再通过fetchLocalBlocks()来获取本地的文件块。</p>
<p>关于ShuffleBlockFetcherIterator有一个非常优雅的点，就是它继承了Iterator类，自己实现了next()方法。在这个方法里面，如果当前获取的块还没有通过网络拿到，那么就take()等待，如果已经通过网络获取到了，那么就直接返回下一个文件块。</p>
<p>下面继续回到BlockStoreShuffleReader的read方法里面，在数据全部拉取结束后，判断一下是否定义了aggregator，如果有的话，先看一下是否有mapSideCombine属性，如果有的话，就调用combineCombinersByKey来进行结果计算；如果没有mapSideCombine属性，就直接combineValuesByKey来进行结果计算。</p>
<p>如果定义了keyOrdering的属性，那么创建一个ExternalSorter对象，用来给所有的数据进行外部排序。</p>
<p>总结一下：由于Spark的lazy的计算模型，在Task在Executor端被执行的时候，才会真正的开始进行计算，也正在在这个逻辑里面，对于ShuffledRDD会首先去进行自己的数据拉取，ShuffleRead的逻辑已经在很久之前写在自己的compute方法里面了。</p>
<h1 id="7-结束点"><a href="#7-结束点" class="headerlink" title="7. 结束点"></a>7. 结束点</h1><p>spark.stop()是整个程序的结束点，在SparkSession里面调用的stop方法，会直接调用sparkContext里面的stop的方法。如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def stop(): Unit = &#123;</span><br><span class="line">  sparkContext.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>SparkSession里面调用的stop方法会进行各种各样的清理工作。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/spark-%E6%BA%90%E7%A0%81/" rel="tag"># spark, 源码</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/17/TCP%E7%BD%91%E7%BB%9C%E6%8B%A5%E5%A1%9E%E6%8E%A7%E5%88%B6/" rel="prev" title="TCP网络拥塞控制">
                TCP网络拥塞控制 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/yoelee" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:li.ya.kun@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%B0%8F%E7%A8%8B%E5%BA%8F-GroupByTest"><span class="nav-number">1.</span> <span class="nav-text">1. 小程序-GroupByTest</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E5%88%87%E5%85%A5%E7%82%B9"><span class="nav-number">2.</span> <span class="nav-text">2. 切入点</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E7%AC%AC%E4%B8%80%E4%B8%AARDD%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="nav-number">3.</span> <span class="nav-text">3. 第一个RDD的创建</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-RDD%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="nav-number">4.</span> <span class="nav-text">4. RDD的转换</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-RDD%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">5.</span> <span class="nav-text">5. RDD持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#5-1-%E5%AE%9A%E4%B9%89%E7%BC%93%E5%AD%98"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 定义缓存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-2-%E8%A7%A6%E5%8F%91%E7%BC%93%E5%AD%98"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 触发缓存</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#6-RDD%E7%9A%84Action"><span class="nav-number">6.</span> <span class="nav-text">6. RDD的Action</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-1-Driver%E5%88%9B%E5%BB%BATaskSet"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 Driver创建TaskSet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-2-Executor%E6%89%A7%E8%A1%8CTask"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 Executor执行Task</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-1-ShuffleMapTask%E7%9A%84%E7%BB%93%E6%9E%9C%E5%86%99%E5%85%A5-Shuffle-Write"><span class="nav-number">6.2.1.</span> <span class="nav-text">6.2.1 ShuffleMapTask的结果写入(Shuffle Write)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-1-SortShuffleWriter"><span class="nav-number">6.2.1.1.</span> <span class="nav-text">6.2.1.1 SortShuffleWriter</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-2-UnsafeShuffleWriter"><span class="nav-number">6.2.1.2.</span> <span class="nav-text">6.2.1.2 UnsafeShuffleWriter</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-3-BypassMergeSortShuffleHandle"><span class="nav-number">6.2.1.3.</span> <span class="nav-text">6.2.1.3 BypassMergeSortShuffleHandle</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-2-%E5%B0%86%E7%BB%93%E6%9E%9C%E9%80%9A%E7%9F%A5Driver"><span class="nav-number">6.2.2.</span> <span class="nav-text">6.2.2 将结果通知Driver</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-3-Driver%E5%A4%84%E7%90%86%E7%BB%93%E6%9E%9C"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 Driver处理结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-4-ShuffleRead"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 ShuffleRead</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#7-%E7%BB%93%E6%9D%9F%E7%82%B9"><span class="nav-number">7.</span> <span class="nav-text">7. 结束点</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">亚坤</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
